{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f0692f-7112-4864-8014-a830c6be7ca9",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc43b0-c2b0-464b-ae96-92648b76b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 1:-Gradient Boosting Regression is a machine learning algorithm used for both regression and classification tasks.\n",
    "It is an ensemble learning technique that builds a predictive model in a stage-wise fashion.\n",
    "Gradient Boosting combines the predictions of multiple weak learners (usually decision trees) to create a strong predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c169e09-112f-4c0f-8202-e96f28bfd79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Initialization:\n",
    "    The algorithm starts with an initial model, often a simple one, that predicts the mean (for regression) of the target variable.\n",
    "\n",
    "Stage-wise Training:\n",
    "    The algorithm then builds a series of weak learners (typically decision trees) in a sequential manner.\n",
    "    Each new weak learner is trained to correct the errors of the combined model built so far.\n",
    "    The emphasis is on the samples that were poorly predicted by the existing model.\n",
    "\n",
    "Gradient Descent Optimization:\n",
    "    The new model is trained to minimize the residual errors of the combined model.\n",
    "    The gradient descent optimization technique is used to find the parameters of the new model (e.g., tree structure, weights) that minimize the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b790bd90-462d-4bbf-aa7b-96f748db44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Key Characteristics of Gradient Boosting Regression:\n",
    "\n",
    "Sequential Building: Learners are built sequentially, each one correcting the errors of the previous one.\n",
    "\n",
    "Model Complexity: The final model can be highly complex, as it is a combination of many weak learners.\n",
    "\n",
    "Robustness: Gradient Boosting is robust to outliers in the data.\n",
    "\n",
    "Hyperparameters: Important hyperparameters include the learning rate, the number of weak learners, and the depth of each weak learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2265034-e931-40a4-b774-0e8aba45e7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89df6de7-c3f1-4402-81eb-d11b031af0a3",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a4c4fd-9b92-4cf7-be9b-24d82135dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 2:-\n",
    "Implementing a gradient boosting algorithm from scratch involves several steps, and its a complex process.\n",
    "Heres a simplified example using a decision stump as a weak learner for a regression problem.\n",
    "This example serves for educational purposes and may not cover all the optimizations and features of a complete implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e868a8c0-75fc-4f80-92c3-6fdf8b4b79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 4 * (X - 0.5) ** 2 + np.random.randn(100, 1) / 10\n",
    "\n",
    "# Simple decision stump as a weak learner\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.left_prediction = None\n",
    "        self.right_prediction = None\n",
    "\n",
    "def split(X, y, feature, value):\n",
    "    left_mask = X[:, feature] < value\n",
    "    right_mask = ~left_mask\n",
    "    return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
    "\n",
    "def mean_squared_error_reduction(y, y_left, y_right):\n",
    "    mse_before = np.mean((y - np.mean(y))**2)\n",
    "    mse_after = (np.sum((y_left - np.mean(y_left))**2) + np.sum((y_right - np.mean(y_right))**2)) / len(y)\n",
    "    return mse_before - mse_after\n",
    "\n",
    "def find_best_split(X, y):\n",
    "    best_feature, best_value, best_reduction = None, None, 0\n",
    "    for feature in range(X.shape[1]):\n",
    "        unique_values = np.unique(X[:, feature])\n",
    "        for value in unique_values:\n",
    "            X_left, y_left, X_right, y_right = split(X, y, feature, value)\n",
    "            reduction = mean_squared_error_reduction(y, y_left, y_right)\n",
    "            if reduction > best_reduction:\n",
    "                best_reduction = reduction\n",
    "                best_feature = feature\n",
    "                best_value = value\n",
    "    return best_feature, best_value\n",
    "\n",
    "def fit_stump(X, y):\n",
    "    stump = DecisionStump()\n",
    "    stump.split_feature, stump.split_value = find_best_split(X, y)\n",
    "    X_left, y_left, X_right, y_right = split(X, y, stump.split_feature, stump.split_value)\n",
    "    stump.left_prediction = np.mean(y_left)\n",
    "    stump.right_prediction = np.mean(y_right)\n",
    "    return stump\n",
    "\n",
    "def predict_stump(stump, X):\n",
    "    return np.where(X[:, stump.split_feature] < stump.split_value, stump.left_prediction, stump.right_prediction)\n",
    "\n",
    "# Gradient Boosting\n",
    "def gradient_boosting(X, y, num_estimators=100, learning_rate=0.1):\n",
    "    y_pred = np.zeros_like(y)\n",
    "    weak_learners = []\n",
    "\n",
    "    for _ in range(num_estimators):\n",
    "        residual = y - y_pred\n",
    "        stump = fit_stump(X, residual)\n",
    "        y_pred += learning_rate * predict_stump(stump, X)\n",
    "        weak_learners.append(stump)\n",
    "\n",
    "    return weak_learners\n",
    "\n",
    "# Prediction\n",
    "def predict_gb(X, weak_learners):\n",
    "    y_pred = np.zeros(X.shape[0])\n",
    "    for stump in weak_learners:\n",
    "        y_pred += stump.left_prediction * (X[:, stump.split_feature] < stump.split_value)\n",
    "        y_pred += stump.right_prediction * (X[:, stump.split_feature] >= stump.split_value)\n",
    "    return y_pred\n",
    "\n",
    "# Train Gradient Boosting\n",
    "weak_learners = gradient_boosting(X, y, num_estimators=100, learning_rate=0.1)\n",
    "\n",
    "# Evaluate on training data\n",
    "y_pred = predict_gb(X, weak_learners)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X, y, label='Actual')\n",
    "plt.scatter(X, y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "print(f'R-squared: {r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45b77ca-7107-4e75-92b3-1ab24ce2c7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "039ca546-6633-43c9-a597-4ac128893448",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58256a8d-5d07-4ff2-9925-8643fbf34b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 3:-\n",
    "Performing grid search or random search for hyperparameter tuning involves trying different combinations of hyperparameters and evaluating their performance on a \n",
    "validation set. \n",
    "Heres an example of how you might do this using Scikit-Learn's GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f6c32-8e57-4a36-a1f2-42e5ac8ba025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming you have X_train, y_train, and X_val, y_val datasets\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'num_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create the model\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(gb_model, param_grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train a model with the best hyperparameters on the full training set\n",
    "best_gb_model = GradientBoostingRegressor(**best_params)\n",
    "best_gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred = best_gb_model.predict(X_val)\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f'Best Hyperparameters: {best_params}')\n",
    "print(f'Mean Squared Error on Validation Set: {mse_val:.4f}')\n",
    "print(f'R-squared on Validation Set: {r2_val:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662070c1-f018-49ce-8f1a-3ba173f10fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65d06de9-285a-4e8e-8f69-40653201426b",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdb558e-4149-409d-88db-bfe9b6160f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 4:-\n",
    "A weak learner in the context of gradient boosting is a model that performs slightly better than random chance on a binary classification problem. \n",
    "Its typically a simple model, often a shallow decision tree (a decision stump), that has limited predictive power.\n",
    "\n",
    "The idea behind gradient boosting is to combine the outputs of multiple weak learners to create a strong learner. \n",
    "In each iteration of the boosting process, a new weak learner is trained to correct the errors made by the existing ensemble. \n",
    "The contribution of each weak learner is weighted based on its performance.\n",
    "\n",
    "Weak learners are usually chosen to be models that are just slightly better than random guessing. \n",
    "Despite their simplicity, when combined in an ensemble, they can collectively produce a highly accurate predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2c86f-f2e1-4440-9872-0a097901cb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4d15307-1410-44ba-83b7-8a0c8229e8b8",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd98523-b652-4817-b813-50e6c1336aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 5:-\n",
    "The intuition behind the Gradient Boosting algorithm is to sequentially train weak learners, each of which corrects the errors of the previous one.\n",
    "Heres a step-by-step intuition:\n",
    "\n",
    "Start with a Weak Learner: \n",
    "    The algorithm begins with a simple model, often a shallow decision tree (a decision stump). \n",
    "    This initial model provides a baseline prediction.\n",
    "\n",
    "Calculate Residuals: \n",
    "    Calculate the residuals, which are the differences between the actual and predicted values. \n",
    "    These residuals represent the errors of the current model.\n",
    "\n",
    "Train a New Weak Learner: \n",
    "    Train a new weak learner (e.g., another shallow tree) to predict the residuals. \n",
    "    The goal is to correct the errors made by the previous model.\n",
    "\n",
    "Combine Predictions:\n",
    "    Combine the predictions of all weak learners, giving more weight to models that perform better.\n",
    "    The combination is typically done by adding the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7c0f9-b5bd-4472-a451-1da019e242a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f921e0d4-4304-457f-a847-076ed01300e2",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1862eb1-2c62-4208-b6c6-745d631447ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 6:-\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. \n",
    "Heres a step-by-step explanation:\n",
    "\n",
    "Start with a Weak Learner: \n",
    "    The process begins by training a simple model, often a shallow decision tree, to make predictions. \n",
    "    This initial model serves as the first member of the ensemble.\n",
    "\n",
    "Calculate Residuals: \n",
    "    After making predictions with the current model, the algorithm calculates the residuals, which are the differences between the predicted and actual values.\n",
    "\n",
    "Train a New Weak Learner: \n",
    "    The next step is to train a new weak learner to predict these residuals. \n",
    "    This new model focuses on correcting the errors made by the previous one.\n",
    "\n",
    "Combine Predictions: \n",
    "    Combine the predictions of all the weak learners trained so far. \n",
    "    The combination is usually done by adding up the predictions, with each weak learners prediction weighted according to its performance.\n",
    "\n",
    "Update Predictions:\n",
    "    Update the overall predictions by adding the weighted predictions of the new weak learner. \n",
    "    This step adjusts the ensembles predictions to minimize the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea6cd37-4124-48e9-974e-9aaf0fc83518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a33dc7cd-0e10-4f6a-97c8-5f4f7ed8eb54",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71c153-bd24-42dd-8ab3-92635927b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 7:-\n",
    "The mathematical intuition behind the Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "Initialize with a Constant Prediction:\n",
    "Start with a simple model, often a constant prediction, which is the mean (or median) of the target variable.\n",
    "Calculate Residuals:\n",
    "Calcuate the residuals by subtracting the current prediction from the actual target values.\n",
    "Fit a Weak Learner to Residuals:\n",
    "Train a weak learner (usually a shallow decision tree) to predict the residuals. The goal is to capture the errors made by the current model.\n",
    "Update Predictions:\n",
    "Update the predictions by adding the predictions of the weak learner, scaled by a small learning rate (shrinkage parameter). \n",
    "This step adjusts the overall prediction towards the correct values.\n",
    "Repeat:\n",
    "Repeat steps 2-4 for a specified number of iterations or until a convergence criterion is met.\n",
    "Final Prediction:\n",
    "The final prediction is the sum of the predictions from all weak learners."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
