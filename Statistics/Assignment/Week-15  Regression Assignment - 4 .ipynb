{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f0720c-f8ab-41eb-b103-d92fd5605381",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e53fab2-ab29-4a13-bcc3-0df881cff441",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 1:-Lasso Regression, short for \"Least Absolute Shrinkage and Selection Operator,\" is a linear regression technique that extends ordinary least squares (OLS)\n",
    "regression by adding an L1 regularization term to the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c6675-772d-46fc-825d-ac61aa747ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization Term:\n",
    "    Lasso Regression introduces an L1 regularization term, which is the absolute sum of the regression coefficients, to the cost function.\n",
    "    This regularization term penalizes the absolute magnitude of the coefficients and encourages some coefficients to become exactly zero.\n",
    "    \n",
    "Feature Selection:\n",
    "    Lasso Regression is often used for feature selection.\n",
    "    The L1 regularization term in Lasso tends to produce sparse models, meaning that it encourages many coefficients to be exactly zero.\n",
    "    As a result, it effectively selects a subset of the most relevant features while setting others to zero.\n",
    "    \n",
    "Bias-Variance Trade-Off:\n",
    "    Lasso Regression, like Ridge Regression, provides a bias-variance trade-off.\n",
    "    The regularization term adds bias to the model but reduces its variance.\n",
    "    The choice of the regularization strength (lambda or alpha) determines the balance between bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4261bcb4-40e4-42c0-8b75-8f4aa6677dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "Disadvantages:\n",
    "    Lasso Regression may not perform well when there are many features, and most of them are important.\n",
    "    It tends to shrink coefficients to zero, which can lead to underfitting.\n",
    "    In such cases, Ridge Regression might be a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf262e-cac0-4fbc-908d-599cb1952504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e399bb17-c051-4455-9b3b-0f28458209cd",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c710c78-c3c6-48fe-924a-a9b09f3de239",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 2:-\n",
    "The main advantage of using Lasso Regression in feature selection is its ability to automatically identify and select the most relevant features while discarding\n",
    "less important ones.\n",
    "This advantage arises from the unique characteristics of Lasso Regression, particularly the L1 regularization term, which encourages sparsity in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52953cbe-549e-4d84-b5dd-25b5a517f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Automatic Feature Selection:\n",
    "    Lasso Regression automatically performs feature selection by shrinking the coefficients of certain features to exactly zero.\n",
    "    This means that some features are entirely excluded from the model, effectively reducing the dimensionality of the problem.\n",
    "    \n",
    "Simplicity and Interpretability:\n",
    "    The resulting model from Lasso Regression is often simpler and more interpretable because it includes fewer features.\n",
    "    Simplicity is especially valuable when the dataset contains numerous features, many of which may be irrelevant or redundant.\n",
    "    \n",
    "Enhanced Model Efficiency:\n",
    "    Smaller feature sets resulting from Lasso feature selection can lead to faster model training and prediction times.\n",
    "    This is particularly useful in situations where computational resources or real-time predictions are critical.\n",
    "    \n",
    "Reduced Risk of Overfitting:\n",
    "    Lasso Regression helps prevent overfitting by eliminating features that may not generalize well to new data.\n",
    "    Overfitting occurs when a model learns to fit noise or random fluctuations in the training data, which can lead to poor performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e7723-df2e-4611-a64d-69b1ca2abdf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f082277-3692-46f9-a195-b144b281a18b",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4e2d5-e27f-4102-a5a2-029ed2d4b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 3:-\n",
    "Interpreting the coefficients of a Lasso Regression model is similar to interpreting the coefficients in a standard linear regression model, but with the added \n",
    "consideration of feature selection due to Lassos L1 regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a42ff5-ce58-44e4-af04-941349f3116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Non-Zero Coefficients:\n",
    "    In Lasso Regression, some coefficients will be non-zero, indicating that these features were selected by the model as important for making predictions.\n",
    "    These coefficients represent the estimated effect of each selected feature on the target variable.\n",
    "    \n",
    "Zero Coefficients:\n",
    "    Lasso Regression sets some coefficients to exactly zero, indicating that these features were not deemed important by the model and are effectively excluded from\n",
    "    the final model.\n",
    "    This feature selection aspect is one of the key benefits of Lasso.\n",
    "    \n",
    "Magnitude of Coefficients:\n",
    "    The magnitude of non-zero coefficients indicates the strength and direction of the relationship between the corresponding feature and the target variable.\n",
    "    A positive coefficient means that an increase in the feature value leads to an increase in the predicted target variable, while a negative coefficient implies \n",
    "    the opposite.\n",
    "    \n",
    "Feature Importance:\n",
    "    Features with non-zero coefficients are considered important predictors in the Lasso model.\n",
    "    The magnitude of the coefficient provides a measure of the features importance.\n",
    "    Larger coefficients suggest that a feature has a more substantial impact on the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583ff2c-7b92-40bb-9ed9-790763998c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8288672-dbe7-4d4d-af7b-7cb163a74ab8",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ee8091-ccd9-47d1-90e5-42f9c388f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 4:-\n",
    "In Lasso Regression, there is typically one primary tuning parameter, which is also known as the regularization strength or lambda (λ).\n",
    "This parameter controls the amount of regularization applied to the model.\n",
    "Regularization is essential in Lasso Regression to prevent overfitting and perform feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b8fd7-8c4f-430b-8699-ac3ec9d2f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization Strength (λ):\n",
    "    Effect on Coefficients:\n",
    "        The most significant impact of the regularization strength is on the coefficients of the model.\n",
    "        As λ increases, the magnitude of the coefficients is shrunk toward zero.\n",
    "        This leads to some coefficients becoming exactly zero, effectively excluding certain features from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58687bee-b116-4a8e-8396-2b66bc25f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "Alpha (α) (Elastic Net Only):\n",
    "    In Elastic Net Regression, there is an additional tuning parameter called alpha (α), which combines L1 (Lasso) and L2 (Ridge) regularization.\n",
    "    The choice of α controls the trade-off between L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296ae1a-67a0-4a4b-aac3-dd2223580604",
   "metadata": {},
   "outputs": [],
   "source": [
    "Max Iterations:\n",
    "    The maximum number of iterations or epochs can also be a tuning parameter.\n",
    "    It determines how many iterations the optimization algorithm (e.g., coordinate descent) runs to find the optimal coefficients.\n",
    "    If the algorithm does not converge within the specified number of iterations, you may need to increase this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec138d-18a9-4242-82fd-8b6a75ec0c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "Convergence Criteria:\n",
    "    The convergence criteria, such as the tolerance level, can be adjusted to determine when the optimization algorithm should stop.\n",
    "    A smaller tolerance may require the algorithm to converge to a more precise solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c4b495-52a2-40cb-b13c-b583f2629334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac809f63-0064-4dc8-8ed5-dd27426dd6c1",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8c7fd9-13bf-4b5b-b53e-d655fcd450b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 5:-\n",
    "Lasso Regression is primarily designed for linear regression problems.\n",
    "It adds L1 regularization to the linear regression model, which encourages sparsity in the coefficients, effectively performing feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d0079f-3935-4866-8d24-8e722c955b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Polynomial Features:\n",
    "    One approach to tackle non-linear relationships is to create polynomial features from the original features. \n",
    "\n",
    "Interaction Terms:\n",
    "    In some cases, non-linear relationships can be captured by introducing interaction terms between the features.\n",
    "    \n",
    "Feature Engineering:\n",
    "    Feature engineering is a crucial step in addressing non-linear regression problems.\n",
    "    \n",
    "Feature Engineering:\n",
    "    Feature engineering is a crucial step in addressing non-linear regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e923d-e38a-42e4-a405-1e4a0411d7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09ad2b05-0609-47c3-bf1d-47cb50942ed5",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c471c-abc9-463a-956b-6d60d4b1a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 6:-\n",
    "Ridge Regression and Lasso Regression are both regularization techniques used in linear regression to prevent overfitting and improve the generalization of the model.\n",
    "While they share the goal of reducing model complexity, they differ in how they achieve this and in their specific effects on the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693dc11-9e12-417f-88f8-b02d63c80223",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization Technique:\n",
    "    Ridge Regression:\n",
    "        Also known as L2 regularization, Ridge Regression adds a penalty term that is proportional to the square of the magnitude of the coefficients.\n",
    "        This penalty encourages the coefficients to be small but not exactly zero.\n",
    "        \n",
    "Lasso Regression:\n",
    "    Also known as L1 regularization, Lasso Regression adds a penalty term that is proportional to the absolute values of the coefficients.\n",
    "    This penalty encourages sparsity in the model, effectively setting some coefficients to exactly zero.\n",
    "    \n",
    "Effect on Coefficients:\n",
    "    Ridge Regression:\n",
    "        Ridge shrinks the coefficients toward zero but doesnt force them to be exactly zero.\n",
    "        It reduces the magnitude of all coefficients, preventing overfitting by making the model less sensitive to individual data points.\n",
    "        \n",
    "Lasso Regression:\n",
    "    Lasso can set some coefficients to exactly zero, effectively performing feature selection.\n",
    "    It eliminates certain features from the model, making it more interpretable and simplifying it.\n",
    "    \n",
    "Bias-Variance Trade-off:\n",
    "    Ridge Regression:\n",
    "        Ridge reduces variance more than bias.\n",
    "        Its suitable when many features are relevant, and it helps prevent multicollinearity by distributing the impact among correlated features.\n",
    "        \n",
    "Lasso Regression:\n",
    "    Lasso reduces both variance and bias.\n",
    "    It is suitable when there are many features, but only a subset of them is relevant.\n",
    "    Lasso performs feature selection and yields a simpler model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02eb5a-ad85-4771-888e-d1acbbe77b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "857b0591-9a9d-4681-aa5a-206db0bb002e",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c976b51-8cf9-434c-87ea-6d3bb765d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 7:-\n",
    "Yes, Lasso Regression can handle multicollinearity in the input features to some extent, although its primary focus is on feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c9ee04-76c0-4df6-8785-ac438a31dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature Selection:\n",
    "    Lasso Regression is known for its ability to perform feature selection by setting some coefficients to exactly zero.\n",
    "    In the presence of multicollinearity, Lasso tends to select one of the correlated features and set the coefficients of the others to zero.\n",
    "    This process effectively eliminates redundant or highly correlated features from the model.\n",
    "\n",
    "Reduced Model Complexity:\n",
    "    By eliminating some features, Lasso reduces the complexity of the model.\n",
    "    This can be especially beneficial when dealing with a high-dimensional dataset with many irrelevant or redundant features, as it simplifies the model and can\n",
    "    improve its interpretability.\n",
    "\n",
    "Improved Generalization:\n",
    "    Removing unnecessary features through Lasso can lead to a model that is less prone to overfitting.\n",
    "    With fewer features to fit, the model may generalize better to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23a847-1830-4177-891c-59c9d27bf8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fda8cff-4ca2-4d7c-9a64-7a22b1ef06b9",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6865c-6ef8-4f7d-ba2e-ec76bb5f0a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 8:-\n",
    "Choosing the optimal value of the regularization parameter (lambda, λ) in Lasso Regression is a crucial step to ensure that the model achieves the right balance\n",
    "between model complexity and overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f832dd9e-62fd-4380-8f05-3790e6670b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cross-Validation:\n",
    "    Cross-validation is a widely used technique for selecting the optimal λ in Lasso Regression.\n",
    "    The most common method is k-fold cross-validation, where you divide your dataset into k subsets (folds).\n",
    "    The steps are as follows:\n",
    "\n",
    "    Divide the dataset into k subsets (folds).\n",
    "    Train the Lasso Regression model with different values of λ on (k-1) folds.\n",
    "    Evaluate the models performance on the remaining fold.\n",
    "    Repeat this process for each fold, using a different fold as the validation set each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426ca10-084e-4ba9-b16b-7096d4bc7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid Search:\n",
    "    Perform a grid search over a range of λ values.\n",
    "    You specify a list of potential λ values to test, and the grid search algorithm systematically trains Lasso Regression models with each value. \n",
    "    \n",
    "Information Criteria:\n",
    "    Some information criteria, such as Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC), can be used to estimate the quality of a\n",
    "    model based on its likelihood and the number of parameters (features). \n",
    "    \n",
    "Plotting the Validation Curve:\n",
    "    Create a validation curve by plotting the models performance metric (e.g., Mean Squared Error) against different λ values.\n",
    "    Look for the λ value at which the performance metric is minimized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
