{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "264789a0-2411-45a0-9de2-7911797d517f",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60760770-4b3e-4b8a-b5fe-45a9027d6871",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 1:-Ensemble techniques in machine learning involve combining the predictions from multiple models to create a stronger, more robust, and often more accurate model \n",
    "than the individual models. \n",
    "The idea is that by combining the predictions of multiple models, the weaknesses of one model can be compensated for by the strengths of others, leading to improved\n",
    "overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc713a-2049-4760-a2fb-f0c1f7ab85a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "The two main types of ensemble techniques are:\n",
    "\n",
    "Bagging (Bootstrap Aggregating): \n",
    "    In bagging, multiple instances of the same learning algorithm are trained on different subsets of the training data.\n",
    "    Each model gives its prediction, and the final prediction is often an average (for regression problems) or a majority vote (for classification problems) of the \n",
    "    individual predictions.\n",
    "\n",
    "Example: \n",
    "    Random Forest is an ensemble learning method based on bagging.\n",
    "    It builds multiple decision trees and merges them together to get a more accurate and stable prediction.\n",
    "    \n",
    "Boosting: \n",
    "    In boosting, models are trained sequentially, with each model trying to correct the errors made by the previous ones. \n",
    "    The focus is on instances that were misclassified by earlier models, and their weights are adjusted to give more importance to the difficult-to-classify instances.\n",
    "\n",
    "Example: \n",
    "    AdaBoost (Adaptive Boosting) is a popular boosting algorithm. \n",
    "    It assigns weights to data points and fits a sequence of weak learners (usually shallow decision trees) with the goal of improving the overall model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac29b7-7abb-40a9-a383-1cef8e0f9483",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ad7f91-bd06-4b91-bebf-7a351a1e2a23",
   "metadata": {},
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b8d879-c1e3-4047-9248-5770eddd1bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 2:-\n",
    "Improved Accuracy: \n",
    "    Ensemble methods often lead to improved accuracy compared to individual models. \n",
    "    By combining the predictions of multiple models, the strengths of one model can compensate for the weaknesses of another, resulting in a more robust and accurate\n",
    "    overall prediction.\n",
    "\n",
    "Reduced Overfitting: \n",
    "    Ensemble methods can reduce overfitting, especially in complex models.\n",
    "    Overfitting occurs when a model performs well on the training data but poorly on new, unseen data. \n",
    "    Ensemble methods, particularly bagging, can help reduce overfitting by averaging or combining predictions, making the model more generalizable.\n",
    "\n",
    "Increased Robustness: \n",
    "    Ensemble methods are more robust to outliers and noise in the data.\n",
    "    Outliers may have a strong impact on individual models, but when combined with other models in an ensemble, their influence tends to be mitigated.\n",
    "\n",
    "Handling Model Complexity: \n",
    "    Ensemble methods can handle complex relationships in the data. \n",
    "    By combining different models, each capturing different aspects of the data, ensembles can model complex patterns and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc32627a-a0d9-43a9-813b-ad729eaabf53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a02b8667-ba01-42e2-86f7-a29376fe4a76",
   "metadata": {},
   "source": [
    "Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b905299-eadc-4d14-9c3a-7d1383c506cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 3:-\n",
    "Bagging, which stands for Bootstrap Aggregating, is an ensemble technique in machine learning designed to improve the stability and accuracy of machine learning \n",
    "algorithms, particularly decision trees. \n",
    "The main idea behind bagging is to create multiple subsets of the training dataset through resampling (with replacement) and then train a separate model on each \n",
    "subset. \n",
    "The predictions from these models are then combined, often by averaging for regression tasks or voting for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8791a-70cd-49aa-9054-94b12b008f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "260f6d0e-fa44-4102-8773-4a47824a3690",
   "metadata": {},
   "source": [
    "Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7bd46-8a88-415f-9b32-bb9045d1046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 4:-Boosting is another ensemble technique in machine learning that combines multiple weak learners to create a strong learner. \n",
    "Unlike bagging, which trains each model independently and combines their predictions, boosting focuses on training models sequentially, where each new model corrects\n",
    "the errors of its predecessor. \n",
    "The idea is to give more weight to misclassified instances, allowing subsequent models to pay more attention to the challenging cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d76fb48-1ed4-4873-81d8-42f73e2981c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b11f5cd-99e9-4d6a-a291-5bc555cc7407",
   "metadata": {},
   "source": [
    "Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b92b332-9aeb-4be8-b0f4-33fff264a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 5:-\n",
    "Improved Accuracy: \n",
    "    One of the main advantages of ensemble methods is their ability to improve the accuracy of predictions. \n",
    "    By combining the outputs of multiple models, ensemble techniques can overcome the limitations of individual models and provide more robust predictions.\n",
    "\n",
    "Reduction of Overfitting:\n",
    "    Ensemble methods often reduce overfitting, especially when using techniques like bagging.\n",
    "    By training multiple models on different subsets of the data or using different algorithms, ensembles can produce more generalized models that perform well on\n",
    "    unseen data.\n",
    "\n",
    "Increased Robustness:\n",
    "    Ensembles are less sensitive to outliers and noise in the data. \n",
    "    Even if individual models make errors on specific instances, the ensembles overall performance tends to be more robust.\n",
    "\n",
    "Handling Complex Relationships: \n",
    "    Ensemble methods can capture complex relationships in the data that may be challenging for a single model to learn. \n",
    "    This is particularly true for tree-based ensembles like Random Forest and Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d96bb-9ca6-4809-af59-fbb90d53f11a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c89b8d25-b990-451d-803e-89b1a1b0550f",
   "metadata": {},
   "source": [
    "Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012da5b3-db66-410a-9560-371bfb6f9dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 6:-\n",
    "Data Quality: \n",
    "    If the dataset is small, noisy, or lacks diversity, ensembles might not be as effective.\n",
    "    Ensemble methods thrive on diverse and complementary models, so if the base models are all similar or perform poorly, the ensembles performance might not improve.\n",
    "\n",
    "Computational Resources: \n",
    "    Ensemble methods, especially those involving a large number of models like Random Forest or boosting algorithms, can be computationally expensive.\n",
    "    In situations where computational resources are limited, training and maintaining an ensemble might not be practical.\n",
    "\n",
    "Model Selection: \n",
    "    The choice of base models is crucial.\n",
    "    If all models in the ensemble are weak or highly correlated, the ensemble may not provide the desired improvement.\n",
    "    Careful selection and tuning of base models are essential.\n",
    "\n",
    "Training Time:\n",
    "    Ensemble methods can take longer to train than individual models, particularly if the base models are complex or require extensive tuning. \n",
    "    In time-sensitive applications, this increased training time could be a drawback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08273f3-dda6-4f30-99b0-fc8f7360f35f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91ac31bf-b4fa-4655-a60d-cda244619b45",
   "metadata": {},
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f33a55-f586-4756-bd2c-3911b8cf50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 7:-The confidence interval (CI) using bootstrap resampling involves generating multiple bootstrap samples from the original dataset, calculating the statistic of \n",
    "interest for each sample, and then using the distribution of these statistics to estimate the confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a0a1df-e5ef-4dc9-a375-a440e8aca909",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Resampling (Bootstrap Sampling): Randomly sample, with replacement, from the original dataset to create multiple bootstrap samples. \n",
    "These samples should be the same size as the original dataset.\n",
    "\n",
    "Statistic Calculation: \n",
    "    For each bootstrap sample, calculate the statistic of interest (mean, median, standard deviation, etc.).\n",
    "\n",
    "CI Calculation: \n",
    "    Determine the desired confidence level (e.g., 95%). \n",
    "    Sort the calculated statistics and find the values that correspond to the lower and upper percentiles of the distribution. \n",
    "    These values become the lower and upper bounds of the confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3979b830-4727-4e1e-a1bf-e9ac34e454c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Confidence Interval: [3.8 7.3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "num_samples = 1000\n",
    "\n",
    "bootstrap_samples = [np.random.choice(data, size=len(data), replace=True) for _ in range(num_samples)]\n",
    "means = [sample.mean() for sample in bootstrap_samples]\n",
    "\n",
    "confidence_interval = np.percentile(means, [2.5, 97.5])\n",
    "\n",
    "print(\"Bootstrap Confidence Interval:\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea4986-77cf-4c4d-9d45-9b7e3201a1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75483856-c802-41c6-8544-9ca618c89e53",
   "metadata": {},
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e3135-9908-483a-914f-e93c2ef5ee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 8:-\n",
    "Bootstrap is a statistical resampling technique used to estimate the distribution of a statistic by repeatedly resampling with replacement from the observed data. \n",
    "The primary goal is to infer characteristics of the population distribution, such as the mean or confidence intervals, based on the sample data. \n",
    "Here are the steps involved in the bootstrap method:\n",
    "\n",
    "Original Sample:\n",
    "    Start with an original dataset of size of n.\n",
    "    \n",
    "Resampling with Replacement:\n",
    "Randomly select n data points from the original dataset with replacement. \n",
    "This means that a single data point can be selected multiple times, or not at all, in each bootstrap sample.\n",
    "\n",
    "Statistical Calculation:\n",
    "    Calculate the statistic of interest (mean, median, standard deviation, etc.) for each bootstrap sample. \n",
    "    This creates a distribution of the statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c82f4c-1d16-4a56-9aaa-0f67b68a90ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Confidence Interval: [3.8 7.2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "num_samples = 1000\n",
    "\n",
    "bootstrap_samples = [np.random.choice(data, size=len(data), replace=True) for _ in range(num_samples)]\n",
    "\n",
    "means = [sample.mean() for sample in bootstrap_samples]\n",
    "\n",
    "confidence_interval = np.percentile(means, [2.5, 97.5])\n",
    "\n",
    "print(\"Bootstrap Confidence Interval:\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a1202f-6f95-4d65-b567-dce01b88ae80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca512bd5-7115-4659-979b-8f6df80d5aaa",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af4f5234-fa9c-4abe-92b4-2e7c96d1cd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Confidence Interval for Mean Height: [14.85788497 16.10718356]\n"
     ]
    }
   ],
   "source": [
    "# Ans 9\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "sample_heights = np.random.normal(loc=15, scale=2, size=50)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_samples = 10000\n",
    "\n",
    "# Bootstrap resampling\n",
    "bootstrap_samples = [np.random.choice(sample_heights, size=len(sample_heights), replace=True) for _ in range(num_samples)]\n",
    "\n",
    "# Calculate mean for each bootstrap sample\n",
    "means = [sample.mean() for sample in bootstrap_samples]\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval = np.percentile(means, [2.5, 97.5])\n",
    "\n",
    "print(\"Bootstrap Confidence Interval for Mean Height:\", confidence_interval)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
