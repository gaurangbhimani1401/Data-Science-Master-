{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10130a1-fd3c-47be-b407-58c643c50b5d",
   "metadata": {},
   "source": [
    "Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach?\n",
    "Explain with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906b2962-b7bf-4981-9057-4533ffbd9465",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 1:-Eigenvalues and eigenvectors are concepts from linear algebra that play a crucial role in various mathematical and computational applications, including the \n",
    "eigen-decomposition approach used in Principal Component Analysis (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6078341-96f6-4dd3-9038-f7b33e06c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Standardization:\n",
    "    PCA typically starts with standardizing the data, which means transforming the data to have zero mean and unit variance. \n",
    "    This step ensures that all features are on a similar scale.\n",
    "Covariance Matrix:\n",
    "    PCA involves calculating the covariance matrix of the standardized data. \n",
    "    The covariance matrix captures the relationships and variances between different features.\n",
    "election of Principal Components:\n",
    "    PCA selects principal components based on the magnitudes of their associated eigenvalues.\n",
    "    The larger the eigenvalue, the more variance is captured along the corresponding principal component.\n",
    "    High-variance dimensions will likely have higher eigenvalues, making them more likely to be selected as principal components.\n",
    "Dimensionality Reduction:\n",
    "    By selecting a subset of the principal components that capture a significant portion of the total variance, PCA effectively focuses on the directions of high \n",
    "    variance and reduces the impact of dimensions with low variance.\n",
    "Reduced-Dimensional Representation:\n",
    "    The final step involves projecting the data onto the selected principal components. \n",
    "    This projection results in a reduced-dimensional representation of the data, emphasizing the directions with the highest variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa62a7-4294-408f-988f-623fbf642f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eigenvalues (λ):\n",
    "The characteristic equation is ∣A−λI∣=0, where I is the identity matrix.\n",
    "For matrix A, the characteristic equation is (4−)(3−)−2=0(4−λ)(3−λ)−2=0.\n",
    "Solving this equation gives the eigenvalues λ₁ = 5 and λ₂ = 2.\n",
    "\n",
    "Eigenvectors (x):\n",
    "For each eigenvalue, substitute it back into −A−λI and solve for the eigenvector.\n",
    "For λ₁ = 5, solve (−5)=0(A−5I)x=0 to find the corresponding eigenvector.\n",
    "For λ₂ = 2, solve (−2)=0(A−2I)x=0 to find the corresponding eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f75ab4-dacb-4432-ae4c-d03e4977b2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06c97aa8-257d-4b2e-89fb-7f2ba75f8ae3",
   "metadata": {},
   "source": [
    "Q2. What is eigen decomposition and what is its significance in linear algebra?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c580dc-14b9-4a86-8aff-6b197e98131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 2:-Eigen decomposition, also known as spectral decomposition, is a fundamental concept in linear algebra that involves decomposing a square matrix into a set of\n",
    "its eigenvalues and corresponding eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06658f-d95a-4f95-adda-7a76c2292a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Understanding Matrix Powers:\n",
    "    Eigen decomposition simplifies the computation of matrix powers. \n",
    "    For example, raising a matrix A to a power n can be expressed as =Λ−1A n =PΛ n P −1 , where ΛΛ n involves raising the eigenvalues to the power n.\n",
    "    This simplifies complex matrix exponentiation.\n",
    "Diagonalization:\n",
    "    Eigen decomposition diagonalizes a matrix, expressing it in terms of its eigenvalues and eigenvectors. \n",
    "    A diagonal matrix is often easier to analyze and manipulate than a general matrix. \n",
    "    This property is especially useful in solving systems of linear equations.\n",
    "Understanding Linear Transformations:\n",
    "    Eigen decomposition provides insights into the behavior of linear transformations represented by matrices.\n",
    "    The eigenvectors represent the directions that are only scaled (not rotated) by the transformation, and the eigenvalues represent the scaling factors.\n",
    "Principal Component Analysis (PCA):\n",
    "    In machine learning and statistics, PCA relies on eigen decomposition. \n",
    "    PCA transforms data into a new coordinate system where the dimensions are aligned with the eigenvectors of the covariance matrix. \n",
    "    The eigenvalues indicate the importance of each dimension.\n",
    "Solving Systems of Differential Equations:\n",
    "    Eigen decomposition is used in solving linear systems of ordinary differential equations (ODEs).\n",
    "    It simplifies the process by transforming the system into a set of uncoupled equations.\n",
    "Quantum Mechanics:\n",
    "    In quantum mechanics, eigen decomposition is essential in expressing the state of a quantum system.\n",
    "    The eigenvectors correspond to possible states, and the eigenvalues represent the probabilities associated with those states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa6f1da-b1ba-415d-a3ac-1794c2e35d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bab0b65b-1bc6-4f95-a73f-2e10ac13ef90",
   "metadata": {},
   "source": [
    "Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the\n",
    "Eigen-Decomposition approach? Provide a brief proof to support your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e5a2f-7bc4-4a2e-a344-7b8fa5940ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 3:-For a square matrix A to be diagonalizable, it must satisfy certain conditions.\n",
    "Diagonalization is possible if and only if the matrix has n linearly independent eigenvectors, where n is the size of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a754f-6462-496f-8113-6c9c4edf3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's break down the conditions:\n",
    "\n",
    "Non-defective Matrix:\n",
    "    The matrix A must be non-defective. \n",
    "    A non-defective matrix is one for which all eigenvectors are linearly independent, and there are enough eigenvectors to form a complete set.\n",
    "Full Set of Linearly Independent Eigenvectors:\n",
    "    The matrix A must have n linearly independent eigenvectors.\n",
    "    These eigenvectors form the columns of the matrix P in the eigen decomposition =Λ−1A=PΛP −1. \n",
    "    The matrix P is formed by arranging these eigenvectors as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870988a5-b9d6-4ead-b87c-45f6822c61f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Proof:\n",
    "\n",
    "Assume that A is diagonalizable, meaning that A can be expressed as =Λ−1A=PΛP −1 , where P is the matrix of eigenvectors, and Λ is the diagonal matrix of eigenvalues.\n",
    "\n",
    "Now, lets consider the linear transformation =Av i =λ i v i for each eigenvector v i with eigenvalue λ i If A is diagonalizable, we should be able to express any \n",
    "vector v in terms of the eigenvectors v i:=11+22+v=c 1 v 1 +c2 v 2 +…+cn v n\n",
    "where c i are constants. \n",
    "This is possible because the eigenvectors form a basis for the vector space.\n",
    "\n",
    "Now, lets look at the matrix P formed by the eigenvectors:=[12]P=[v 1​ ,v 2​ ,…,v n​ ]The matrix P is invertible precisely because its columns (eigenvectors) are \n",
    "linearly independent. \n",
    "Thus, −1P −1  exists.\n",
    "\n",
    "So, A is diagonalizable if and only if it has n linearly independent eigenvectors, forming the matrix P, and −1P −1 exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df681883-c721-49b0-a4b7-219fb13e42f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a847cb5c-28c7-46ff-b3b9-efeac8748420",
   "metadata": {},
   "source": [
    "Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach?\n",
    "How is it related to the diagonalizability of a matrix? Explain with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec7435-f185-4e2d-bf69-ade302d290c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 4:-The Spectral Theorem is a fundamental result in linear algebra that establishes a connection between certain classes of matrices and their \n",
    "eigenvalues/eigenvectors.\n",
    "In the context of the Eigen-Decomposition approach, the Spectral Theorem is particularly relevant as it provides conditions under which a matrix is diagonalizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076c6f43-6008-41e1-998f-76e65049446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Spectral Theorem:\n",
    "For a symmetric matrix A (a square matrix that is equal to its transpose, =A=A T ), there exists an orthogonal matrix P (a matrix whose transpose is its inverse,\n",
    "=−1P T =P −1) and a diagonal matrix ΛΛ such that:=ΛA=PΛP T This means that a symmetric matrix can be diagonalized by an orthogonal matrix. \n",
    "The diagonal elements of Λ are the eigenvalues of A, and the columns of P are the corresponding eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6218693b-0792-4ee0-a6d2-e7b8412f097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Significance and Relation to Diagonalizability:\n",
    "Diagonalizability: \n",
    "    The Spectral Theorem provides a specific condition (symmetry) under which a matrix can be diagonalized.\n",
    "    This is significant because diagonalizable matrices are often easier to work with in various mathematical operations.\n",
    "Orthogonality: \n",
    "    The use of an orthogonal matrix P in the diagonalization means that the eigenvectors form an orthonormal basis for the vector space. \n",
    "    This ensures that the transformation represented by A doesnt distort distances or angles, preserving the geometric structure.\n",
    "Eigenvalues and Eigenvectors: \n",
    "    The diagonal matrix ΛΛ contains the eigenvalues of A, which are the scaling factors in the transformation. \n",
    "    The columns of P are the corresponding eigenvectors, indicating the directions along which the scaling occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01783d99-4415-49a7-af0f-1765a4367dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "618ab597-bacf-4c44-a96e-110bf7a9cd7f",
   "metadata": {},
   "source": [
    "Q5. How do you find the eigenvalues of a matrix and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30afa684-56a6-4f7a-aa4e-7cd51c4ab082",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 5:-The eigenvalues of a matrix A are the solutions to the characteristic equation, which is obtained by subtracting λI (where λ is the eigenvalue and I is the \n",
    "identity matrix) from A and finding the determinant.\n",
    "The characteristic equation is given by:det(−)=0 det(A−λI)=0 Solving this equation for λ yields the eigenvalues of the matrix.\n",
    "The eigenvalues represent the scaling factors by which the matrix stretches or compresses space along its eigenvectors.\n",
    "In more practical terms:\n",
    "Scaling Factor: Each eigenvalue λ i corresponds to a particular eigenvector.\n",
    "The magnitude of λ i indicates how much the matrix scales the corresponding eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055fd5e-48ad-4a6d-b6bc-c7b7fa34dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "The eigenvalues represent the scaling factors by which the matrix stretches or compresses space along its eigenvectors.\n",
    "In more practical terms:\n",
    "Scaling Factor: \n",
    "    Each eigenvalue λicorresponds to a particular eigenvector.\n",
    "    The magnitude of λ i  indicates how much the matrix scales the corresponding eigenvector.\n",
    "Change in Scale:\n",
    "    If >1λ i >1, the corresponding eigenvector is scaled up.\n",
    "    If 0<<10<λ i<1, the eigenvector is scaled down. If =1λ i=1, there is no scaling along that eigenvector.\n",
    "Eigenvalue Significance: \n",
    "    The eigenvalues provide essential information about the behavior of linear transformations represented by the matrix.\n",
    "    For example, in the context of diagonalization, they are the diagonal entries of the diagonal matrix Λ in the decomposition =Λ−1A=PΛP −1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6de92f-5821-4faa-909e-81745a276388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6719bcf6-70ce-42dd-90c6-7f4e30c5c2f7",
   "metadata": {},
   "source": [
    "Q6. What are eigenvectors and how are they related to eigenvalues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de345a79-d391-4053-bd64-fd1ab954c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 6:-Eigenvectors are nonzero vectors that, when multiplied by a given square matrix, result in a scalar multiple of itself, with the scalar being the eigenvalue \n",
    "associated with that eigenvector. \n",
    "In mathematical terms, for a square matrix A, a nonzero vector v is an eigenvector corresponding to eigenvalue λ if:\n",
    "    Av=λv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de45fc32-6775-4f8d-931f-645def1225d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here, \n",
    "A is the matrix, v is the eigenvector, and λ is the eigenvalue.\n",
    "Eigenvectors are associated with eigenvalues in the sense that each eigenvector corresponds to a specific eigenvalue for a given matrix. \n",
    "If a matrix has size ×n×n, it will have n eigenvalue-eigenvector pairs.\n",
    "Eigenvectors are often normalized to have a magnitude of 1 for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb78d24-d61b-4497-bf6d-c31a935d7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "The relationship between eigenvectors and eigenvalues is essential in the diagonalization of matrices.\n",
    "For a matrix \n",
    "A with n linearly independent eigenvectors 1,2,…,v 1 ,v 2,…,v n and corresponding eigenvalues 1,2,…,λ 1,λ 2 ,…,λ n, the matrix A can be decomposed as:=\n",
    "A=PΛP −1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddce81b7-3bbe-4bff-98c7-6e9ae381e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "where:\n",
    "    P is a matrix formed by stacking the eigenvectors as columns.\n",
    "    Λ is a diagonal matrix with the corresponding eigenvalues on the diagonal.\n",
    "    This decomposition simplifies matrix operations and is useful in various mathematical and computational applications.\n",
    "    To find the eigenvectors of a matrix, you typically solve the system of linear equations =Av=λv for each eigenvalueλ. \n",
    "    The eigenvectors are not unique; they are determined up to a scalar multiple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd47d4-e4e2-4916-88aa-eec354b6e7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b0354b6-ef37-4f18-8c1c-b671b37d934c",
   "metadata": {},
   "source": [
    "Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e5c908-1142-437d-b4f8-92c3803b1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 7:-The geometric interpretation of eigenvectors and eigenvalues provides insight into the transformation induced by a matrix.\n",
    "Lets consider a 2D matrix for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a87a7a-8583-483b-a861-d540d987941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eigenvalues:\n",
    "    Eigenvalues represent the scaling factor by which the eigenvector is stretched or compressed during a linear transformation.\n",
    "    If λ is a positive eigenvalue, it implies stretching, and if λ is negative, it implies compression.\n",
    "    If =1λ=1, the eigenvector remains unchanged (no stretching or compression).\n",
    "Eigenvectors:\n",
    "    Eigenvectors are directions in space that remain unchanged (up to scaling) after the linear transformation.\n",
    "    The direction of the eigenvector remains fixed, and only its magnitude is scaled by the corresponding eigenvalue.\n",
    "    In a 2D space, an eigenvector might represent the axis along which a figure is stretched or compressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5125de-19a8-4305-bcd6-ae96e15169f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6de7752f-d00c-4ef2-bfde-2171080a7459",
   "metadata": {},
   "source": [
    "Q8. What are some real-world applications of eigen decomposition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b964f-fda4-4be4-8678-b6c7cb0f2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 8:-Eigen decomposition finds applications in various real-world scenarios across different fields.\n",
    "Here are some examples:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd8f7c9-de1f-4076-a07f-ecf48ad922d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Principal Component Analysis (PCA):\n",
    "    PCA uses eigen decomposition to transform a dataset into a new coordinate system, where the axes are the principal components. \n",
    "    This is widely used in dimensionality reduction and feature extraction.\n",
    "Quantum Mechanics:\n",
    "    In quantum mechanics, the eigenstates of a physical system represent the possible states the system can be in, and the corresponding eigenvalues represent the\n",
    "    possible outcomes of measurements.\n",
    "Image Processing:\n",
    "    Eigen decomposition is employed in image processing for techniques such as face recognition.\n",
    "    Eigenvectors can represent key features of images, and eigenvalues indicate the significance of these features.\n",
    "Structural Engineering:\n",
    "    In structural analysis, eigenvalues and eigenvectors are used to analyze the natural frequencies and modes of vibration of structures. \n",
    "    This is crucial for designing structures that can withstand dynamic loads.\n",
    "Chemistry and Molecular Biology:\n",
    "    Quantum chemistry uses eigen decomposition to solve the Schrödinger equation, providing insights into molecular structures and behaviors.\n",
    "Markov Chains:\n",
    "    Eigen decomposition is employed in the study of Markov chains to understand the long-term behavior of a system that undergoes state transitions.\n",
    "Spectral Clustering:\n",
    "    Spectral clustering algorithms utilize eigen decomposition to find a spectral embedding of data, facilitating the discovery of clusters in complex datasets.\n",
    "Finance:\n",
    "    Eigen decomposition is used in portfolio optimization and risk management, where it helps in understanding the relationship between different financial assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77aeef2-fa65-47d2-a3d6-b7bd1101bcfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8d8c98a-7ddd-45b4-8dee-3e67c46e9d88",
   "metadata": {},
   "source": [
    "Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6149e8b-88f8-47dc-9abf-8d04a43ea6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 9:-Yes, a matrix can have more than one set of eigenvectors and eigenvalues.\n",
    "However, each set corresponds to a different decomposition or diagonalization of the matrix.\n",
    "For a square matrix A, if v is an eigenvector and λ is the corresponding eigenvalue, then any scalar multiple of v (i.e., cv, where c is a scalar) is also an \n",
    "eigenvector with the same eigenvalue λ. \n",
    "This means there is an infinite number of eigenvectors associated with a single eigenvalue.\n",
    "Mathematically, if Av=λv, then for any scalar c≠0,A(cv)=λ(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47816dd-8101-4c6e-89b9-7d4f65e3f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Its important to note that different matrices can have the same eigenvalues, but the corresponding eigenvectors may be different.\n",
    "Also, for certain matrices, its possible to have repeated eigenvalues, leading to a situation where there are fewer linearly independent eigenvectors than the \n",
    "algebraic multiplicity of the eigenvalue.\n",
    "In summary, a matrix can have multiple sets of linearly independent eigenvectors, each associated with a unique set of eigenvalues. \n",
    "The total number of distinct eigenvalues (counting multiplicities) is equal to the size of the matrix (the number of rows or columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678312b-3328-4812-a668-b8e63dbaa625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99b74e05-ea84-43f9-a749-e0d78fefd3a3",
   "metadata": {},
   "source": [
    "Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning?\n",
    "Discuss at least three specific applications or techniques that rely on Eigen-Decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ca3397-37fe-4619-be15-9328e2d91aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 10:-The Eigen-Decomposition approach is highly useful in various areas of data analysis and machine learning. \n",
    "Here are three specific applications or techniques that rely on Eigen-Decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20f39e-d531-4c8b-b4a1-3b8527eea6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Principal Component Analysis (PCA):\n",
    "    Application: \n",
    "        PCA is a dimensionality reduction technique used to transform a dataset into a new coordinate system, where the axes are the principal components \n",
    "        (eigenvectors) ordered by their corresponding eigenvalues.\n",
    "    How Eigen-Decomposition is Used: \n",
    "        The covariance matrix of the dataset is decomposed using Eigen-Decomposition, resulting in eigenvectors and eigenvalues. \n",
    "        The eigenvectors become the principal components, and the eigenvalues indicate the variance along each principal component.\n",
    "        By selecting the top eigenvalues and their corresponding eigenvectors, one can reduce the dimensionality of the data while retaining most of its variance.\n",
    "Singular Value Decomposition (SVD):\n",
    "    Application: \n",
    "        SVD is a generalization of Eigen-Decomposition applicable to any rectangular matrix. \n",
    "        It is widely used in various applications, including image compression and collaborative filtering in recommendation systems.\n",
    "    How Eigen-Decomposition is Used:\n",
    "        SVD decomposes a matrix into three matrices, one of which contains the singular values (analogous to eigenvalues) and the left and right singular vectors\n",
    "        (analogous to eigenvectors). \n",
    "        The singular values provide information about the importance of each mode, and the singular vectors determine the directions of these modes. \n",
    "        SVD has applications in data compression and noise reduction.\n",
    "Kernel Principal Component Analysis (Kernel PCA):\n",
    "    Application:\n",
    "        Kernel PCA is an extension of PCA that allows the extraction of nonlinear features in a high-dimensional space.\n",
    "        It is used for nonlinear dimensionality reduction.\n",
    "    How Eigen-Decomposition is Used: \n",
    "        In Kernel PCA, the data is mapped to a high-dimensional feature space using a kernel function.\n",
    "        Eigen-Decomposition is then applied to the kernel matrix in this high-dimensional space. \n",
    "        The resulting eigenvectors represent nonlinear combinations of the original features, providing a more expressive representation of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
