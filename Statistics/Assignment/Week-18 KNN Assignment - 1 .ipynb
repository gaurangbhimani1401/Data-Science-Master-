{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94781f82-8bef-4651-af83-7323aa2e30c4",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15713f-39b3-4a13-818a-60361a407af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 1:-K-Nearest Neighbors (KNN) is a simple, instance-based, and supervised learning algorithm used for classification and regression tasks. \n",
    "In KNN, the idea is to predict the class or value of a new data point by looking at the 'k' closest data points in the training set. \n",
    "The proximity or similarity is typically measured using distance metrics such as Euclidean distance, Manhattan distance, or others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d50222-e5b1-4c9b-9e90-aa7f6b4ffca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "For classification, the majority class among the k neighbors is assigned to the new data point. \n",
    "In regression, the predicted value is often the average or weighted average of the values of the k neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0bb1f-55b4-4c5e-b8db-f75fb265c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "The KNN algorithm doesnt involve a training phase; instead, it memorizes the entire training dataset.\n",
    "This characteristic makes it a lazy learner. \n",
    "The choice of the parameter 'k' influences the algorithms performance — a small 'k' can make the algorithm sensitive to noise, while a large 'k' can make it less \n",
    "sensitive but might smooth out important patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17022dcd-fc59-4f81-809a-73cfef2237d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3be40366-fa58-4fc1-bb2b-a5b7271855c4",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c98de-e86d-4948-a792-de0641d7841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 2:-Choosing the right value for k in K-Nearest Neighbors (KNN) is a critical aspect, and it can significantly impact the performance of the algorithm. \n",
    "Here are some considerations for selecting the value ofk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b14e86-d467-41b7-957c-bf7ea9cda171",
   "metadata": {},
   "outputs": [],
   "source": [
    "Odd vs. Even:\n",
    "    Its often recommended to use an odd value for k to avoid ties in the voting process.\n",
    "    With an odd k, you can avoid situations where theres a draw between two classes.\n",
    "\n",
    "Data Characteristics:\n",
    "    The choice of k should depend on the characteristics of the dataset.\n",
    "    If the dataset has a smooth decision boundary, a smaller k might be appropriate.\n",
    "    In contrast, if the decision boundary is more complex and has fluctuations, a larger k might be better.\n",
    "    \n",
    "Cross-Validation:\n",
    "    Use cross-validation techniques to find the optimal k. \n",
    "    Split your dataset into training and validation sets, and try different values of k. \n",
    "    Choose the one that gives the best performance on the validation set.\n",
    "    \n",
    "Rule of Thumb:\n",
    "    A common rule of thumb is to take the square root of the number of data points in the dataset and use that as k. \n",
    "    This is a heuristic and may not always be optimal, but it can be a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d4be5-4bb5-4134-8614-001d263cba9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec37218d-f3f8-4113-ab9c-3e1da28830dd",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548647bf-47af-435e-9397-369d49f18671",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 3:-The primary difference between K-Nearest Neighbors (KNN) classifier and KNN regressor lies in their output and application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b970e8e1-6f94-40d7-a09b-1e11c92c8708",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN Classifier:\n",
    "    Output: \n",
    "        The output of a KNN classifier is a class label. \n",
    "         It assigns the class label that is most common among the k nearest neighbors of the data point being classified.\n",
    "    Application:\n",
    "        KNN classifiers are used for classification tasks where the goal is to assign a discrete class label to a data point.\n",
    "        \n",
    "KNN Regressor:\n",
    "    Output: \n",
    "        The output of a KNN regressor is a continuous value.\n",
    "        It calculates the average or weighted average of the target values of the k nearest neighbors.\n",
    "    Application: \n",
    "        KNN regressors are used for regression tasks where the goal is to predict a continuous target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d990e39-ee85-405d-bf99-0ffa87e4cf11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdb3ead7-bb67-43ec-8377-8038c224ebd2",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c07515-d5e2-4fce-8ad2-186d9abc5f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 4:-The performance of the K-Nearest Neighbors (KNN) algorithm is typically evaluated using various metrics, depending on whether its a classification or \n",
    "regression task. \n",
    "Here are common evaluation metrics for both scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7150a67-9848-4011-8035-771f9743b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy:\n",
    "    Formula:Number of Correct Predictions/Total Number of Predictions\n",
    "    Description: Measures the overall correctness of the classifier.\n",
    "    \n",
    "Precision:\n",
    "    Formula: True Positives/True Positives + False Positives\n",
    "    Description: Measures the accuracy of positive predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b281b35-df39-459e-8c9b-9a95200d3938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bae1158d-e2ce-4b28-bc0d-95037d321efa",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea1ccf-8e08-4eb0-b7f2-c012ec2adc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 5:-The curse of dimensionality refers to various challenges and phenomena that arise when working with high-dimensional data, particularly in the context of\n",
    "machine learning. \n",
    "In the case of K-Nearest Neighbors (KNN), the curse of dimensionality manifests in several ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5701ff41-fb99-4974-a3c8-a4f1ce73fa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "Increased Sparsity:\n",
    "    As the number of dimensions increases, the available data points become increasingly sparse in the high-dimensional space. \n",
    "    This sparsity can lead to a lack of sufficient neighbors around a given point, making it challenging to identify a meaningful \"nearest\" neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38729d4-afbb-4cd6-ae12-83c4d6ee3ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Increased Computational Complexity:\n",
    "    Calculating distances in high-dimensional spaces is computationally expensive. \n",
    "    As the number of dimensions grows, the distance between points tends to increase, and the computational cost of finding neighbors becomes prohibitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47433283-ff49-482a-af89-8c9212265130",
   "metadata": {},
   "outputs": [],
   "source": [
    "Degradation of Performance:\n",
    "    In high-dimensional spaces, the notion of distance becomes less meaningful.\n",
    "    Points that are close in terms of Euclidean distance might not be close in the relevant feature space. \n",
    "    This can result in a degradation of the performance of distance-based algorithms like KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c6b55-eaa6-4bf9-a29e-e3ca95aff108",
   "metadata": {},
   "outputs": [],
   "source": [
    "Need for More Data:\n",
    "    As the dimensionality increases, the amount of data required to maintain the same level of statistical significance also increases. \n",
    "    Obtaining sufficient data to cover the entire feature space becomes challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c3b4c-c299-44de-8a1a-02da50773785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ce42425-3707-4e10-bc84-7c4b8260c4fa",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22534d9-58fa-41e9-8182-110cba6cf5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 6:-Handling missing values in K-Nearest Neighbors (KNN) involves imputing or filling in the missing values in the dataset before applying the KNN algorithm.\n",
    "Here are some common approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9499e50a-5d42-4919-b8d9-1d43e53876ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Imputation Using Mean, Median, or Mode:\n",
    "    For each feature with missing values, replace the missing values with the mean, median, or mode of that feature. \n",
    "    This is a simple and commonly used method but may not be suitable if the data has outliers.\n",
    "    \n",
    "Imputation Using KNN:\n",
    "    Use the KNN algorithm itself to impute missing values. \n",
    "    For each instance with missing values, find its k-nearest neighbors that have non-missing values for the feature in question.\n",
    "    Take the average (for numerical features) or majority class (for categorical features) of these neighbors and use it to impute the missing value.\n",
    "    \n",
    "Data Imputation Libraries:\n",
    "    Utilize libraries or functions in data analysis tools (e.g., pandas in Python) that provide built-in functionality for imputing missing values based on various \n",
    "    strategies.\n",
    "    \n",
    "Predictive Modeling:\n",
    "    Train a predictive model (e.g., regression or classification) to predict missing values based on other features in the dataset.\n",
    "    Use the trained model to predict and fill in missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029cdca0-f8d2-4131-854d-64b73caee360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4e4ce40-b961-4bdd-9f5b-e665d2ab7357",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0413338-fea3-42eb-991e-df6ab438da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 7:-\n",
    "KNN Classifier:\n",
    "    Task:\n",
    "        Used for classification tasks where the goal is to assign a data point to one of several predefined classes.\n",
    "    Output: \n",
    "        Produces a class label as the output.\n",
    "    Distance Metric: \n",
    "        Commonly uses metrics like Euclidean distance or Manhattan distance to measure the distance between data points.\n",
    "        \n",
    "KNN Regressor:\n",
    "    Task: \n",
    "        Used for regression tasks where the goal is to predict a continuous variable.\n",
    "    Output:\n",
    "        Produces a continuous value as the output.\n",
    "    Distance Metric: \n",
    "        Similar to the classifier, it uses distance metrics to measure the proximity between data points.\n",
    "        \n",
    "Comparison:\n",
    "    Nature of Output: \n",
    "        The primary difference is in the nature of the output: classification (discrete classes) for the classifier and regression (continuous values) for the\n",
    "        regressor.\n",
    "    Evaluation Metrics: \n",
    "        The evaluation metrics used for each type of task are different.\n",
    "        Classification uses metrics related to correct class assignments, while regression uses metrics related to the accuracy of predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306a616d-0fe1-4017-8a89-3acd1c97021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Which One to Choose:\n",
    "    Problem Type: \n",
    "        Choose based on the nature of your problem.\n",
    "        If the target variable is categorical, go for the classifier. \n",
    "        If its continuous, go for the regressor.\n",
    "    Evaluation Goals:\n",
    "        Consider the evaluation goals.\n",
    "        If you are more interested in predicting values accurately, go for regression.\n",
    "        If correct classification is crucial, go for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1188b2-8fa9-42b5-8304-5c1731b8e6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50769fdb-6603-4501-8b32-b3bc3a8ca612",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba1bc53-6001-4bbe-a6f1-afbf24aad185",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 8:-Strengths of KNN:\n",
    "Simple Implementation: \n",
    "    KNN is easy to understand and implement, making it accessible for beginners.\n",
    "Non-parametric: \n",
    "    KNN is a non-parametric algorithm, which means it makes no assumptions about the underlying data distribution.\n",
    "Versatility: \n",
    "    KNN can be used for both classification and regression tasks.\n",
    "Adaptability:\n",
    "    The decision boundaries in KNN are flexible and can adapt to the complexity of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e993b34-a784-413d-9f8b-446a5ad850aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weaknesses of KNN:\n",
    "    Computational Cost:\n",
    "        KNN has a high computational cost during inference, especially as the size of the dataset grows.\n",
    "        Each prediction requires computing distances to all training examples.\n",
    "    Sensitivity to Noise and Outliers:\n",
    "        KNN is sensitive to noisy data and outliers, which can significantly impact the performance.\n",
    "    Curse of Dimensionality: \n",
    "        In high-dimensional spaces, the nearest neighbors might not be as meaningful, leading to degraded performance. \n",
    "        This is known as the curse of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9a20c-6af9-409d-90f7-b8911f1b911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Addressing Weaknesses:\n",
    "    Dimensionality Reduction:\n",
    "        For high-dimensional data, consider dimensionality reduction techniques like PCA to reduce the number of features and mitigate the curse of dimensionality.\n",
    "    Outlier Handling: \n",
    "        Preprocess the data to handle outliers or use robust distance metrics that are less sensitive to extreme values.\n",
    "    Distance Metric Selection: \n",
    "        Experiment with different distance metrics based on the nature of your data.\n",
    "        For example, cosine similarity might be more suitable for text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add25f6-314b-4d43-a9cd-b1cf48519116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c31ff280-88a8-4bbd-9889-6268f4ee3120",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac4ccf-c891-49e3-a52d-aa04e17c3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 9:-Euclidean distance is a measure of straight-line distance between two points in Euclidean space. \n",
    "For two points(x1,y1) and (x2,y2)in a two-dimensional space, the Euclidean distance(dE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f48c27-3071-4827-abce-e3a18aacbbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Differences:\n",
    "Path of Measurement:\n",
    "    Euclidean distance measures the straight-line distance between two points.\n",
    "    Manhattan distance measures the distance along the grid-based path (horizontal and vertical movements).\n",
    "Formula:\n",
    "    Euclidean distance involves squaring the differences, taking the sum, and then taking the square root.\n",
    "    Manhattan distance involves taking the absolute differences and summing them directly.\n",
    "Sensitivity:\n",
    "    Euclidean distance is sensitive to the magnitude and scale of dimensions.\n",
    "    Manhattan distance is less sensitive to the scale of individual dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7b680e-07f3-459e-9c2b-2de8d3470019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e035eece-df35-4686-b0fc-895c57376745",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf6950-bc33-45f4-8dce-9ccb22d37cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 10:-The role of feature scaling in KNN (K-Nearest Neighbors) is crucial for ensuring that all features contribute equally to the distance computations. \n",
    "Since KNN relies on the distance between data points to make predictions, its important to have features on similar scales.\n",
    "Heres why feature scaling is important in KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d3887-b848-45e6-bc4e-0dcd66bd4386",
   "metadata": {},
   "outputs": [],
   "source": [
    "Equal Contribution of Features:\n",
    "        Features with larger scales can dominate the distance computations, making the algorithm sensitive to those features.\n",
    "        Feature scaling ensures that all features contribute equally to the distance metric, preventing bias towards features with larger scales.\n",
    "Distance Metric Consistency:\n",
    "        Scaling maintains the consistency of the chosen distance metric. \n",
    "        For example, if youre using Euclidean distance, it assumes that all dimensions are on the same scale.\n",
    "        Without scaling, the distance metric might not accurately represent the true \"distance\" between points in the feature space.\n",
    "Improves Convergence in Distance-Based Algorithms:\n",
    "        In iterative optimization algorithms (e.g., gradient descent), having features on similar scales can help the algorithm converge faster.\n",
    "        This is particularly relevant in the case of algorithms that internally use distance metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
