{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f256487e-4053-4b6d-88ec-2aa3893c5cfd",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c847a07-d4e9-4ede-8274-50361ae96863",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 1:-\n",
    "The mathematical formula for a linear Support Vector Machine (SVM) can be expressed as follows:\n",
    "\n",
    "Given a dataset with features denoted as xᵢ and corresponding binary class labels yᵢ, where i = 1, 2, ..., n (n is the number of data points), and each feature vector\n",
    "xᵢ has d dimensions (xᵢ ∈ ℝᵈ), the linear SVM aims to find a hyperplane represented by the equation:\n",
    "\n",
    "wᵀx + b = 0\n",
    "\n",
    "Where:\n",
    "\n",
    "w is the weight vector (normal to the hyperplane).\n",
    "x is the feature vector.\n",
    "b is the bias or intercept.\n",
    "The decision function for classifying a new data point, x_new, is determined by the sign of the function wᵀx_new + b:\n",
    "\n",
    "If wᵀx_new + b > 0, then x_new is classified as one class (typically the positive class).\n",
    "If wᵀx_new + b < 0, then x_new is classified as the other class (typically the negative class).\n",
    "The goal of linear SVM is to find the optimal values of w and b to maximize the margin between the classes while minimizing classification errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a361c-19b4-4a1f-ae1a-2c02deb1eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this formulation, yᵢ represents the class labels (+1 or -1), and the constraint enforces that data points are correctly classified on the correct side of the\n",
    "margin (at least 1 unit away from the decision boundary). \n",
    "The margin is defined as the distance between the two parallel hyperplanes wᵀx + b = 1 and wᵀx + b = -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fc607c-3915-40b9-86e4-8bc30c352e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b024a60-f82f-4209-acfa-d8a4f7cdd5a7",
   "metadata": {},
   "source": [
    "Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b46291-bd5d-458e-a072-f4cecc6e5a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 2:-The objective function of a linear Support Vector Machine (SVM) is to find the optimal hyperplane (represented by the weight vector w and the bias or intercept\n",
    "    term b) that maximizes the margin between the classes while minimizing classification errors. \n",
    "    This is typically expressed as a convex optimization problem with constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c68149-00a5-4087-8ae5-d2b9ce4a29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this formulation:\n",
    "\n",
    "w represents the weight vector.\n",
    "b represents the bias or intercept term.\n",
    "xᵢ represents the feature vector of the i-th data point.\n",
    "yᵢ represents the class labels (+1 or -1).\n",
    "The objective is to minimize the L2-norm (Euclidean norm) of the weight vector ‖w‖², which corresponds to maximizing the margin between the classes. \n",
    "The margin is defined as the distance between the two parallel hyperplanes wᵀx + b = 1 and wᵀx + b = -1.\n",
    "By minimizing the L2-norm of w, the SVM encourages the hyperplane to be located as far away from the data points as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3424d7eb-675a-45ca-bc99-069b333c3f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffba04b4-ecfd-4dbc-926b-9b97c9f7e2ed",
   "metadata": {},
   "source": [
    "Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9cbc54-f929-457e-8325-7bd0228da8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 3:-The kernel trick in Support Vector Machines (SVM) is a mathematical technique used to transform data from its original feature space into a higher-dimensional \n",
    "feature space.\n",
    "It allows SVMs to find complex, nonlinear decision boundaries in the original feature space.\n",
    "The key idea is to implicitly compute the dot product (inner product) between data points in the higher-dimensional space without explicitly computing the \n",
    "transformation of the data into that space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc07ced5-a234-4292-a479-2f2f57cac316",
   "metadata": {},
   "outputs": [],
   "source": [
    "Original Feature Space: \n",
    "    In the original feature space, data points may not be linearly separable, meaning you cant draw a single straight line or hyperplane to \n",
    "    separate the classes.\n",
    "\n",
    "Mapping to a Higher-Dimensional Space: \n",
    "    To address this, the kernel trick uses a function, called a kernel function (e.g., polynomial kernel, radial basis function (RBF) kernel), that implicitly maps \n",
    "    the data to a higher-dimensional space where it is more likely to be linearly separable.\n",
    "    \n",
    "Decision Boundary in the Higher-Dimensional Space: \n",
    "    In the higher-dimensional space, a linear decision boundary is constructed to separate the classes, and this boundary can be a complex, nonlinear boundary in the\n",
    "    original feature space.\n",
    "    This is achieved by solving a linear SVM problem in the higher-dimensional space.\n",
    "\n",
    "Predictions in the Original Feature Space:\n",
    "    After training the SVM in the higher-dimensional space, predictions are made for new data points in the original feature space. \n",
    "    The kernel trick ensures that these predictions are based on the dot product with respect to the higher-dimensional space, allowing SVMs to classify data points\n",
    "    in the original space even though they are projected into the higher-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6641ced-fcdd-4e96-87f4-f41677612cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89dcfdca-c1be-4f50-b166-afa11c74d7ea",
   "metadata": {},
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60657faa-0fdb-45b5-9bf1-44280f527856",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 4:-In Support Vector Machines (SVM), support vectors are the data points that are closest to the decision boundary (hyperplane) that separates the classes. \n",
    "They are the critical elements of SVM that determine the position and orientation of the decision boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b102175-b0ae-493e-9b37-f49195149b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Determining the Margin: \n",
    "    Support vectors are the data points that are closest to the decision boundary.\n",
    "    The distance between a support vector and the decision boundary is called the margin.\n",
    "    In an ideal case, the margin should be maximized. \n",
    "    The support vectors define this margin, and the larger the margin, the better the SVM can generalize to unseen data.\n",
    "\n",
    "Influencing the Decision Boundary:\n",
    "    The decision boundary is determined by the support vectors.\n",
    "    Non-support vectors do not affect the decision boundary, only the support vectors are involved in this process. \n",
    "    This means that most of the data points are irrelevant to the SVMs decision boundary, making SVMs efficient and robust.\n",
    "\n",
    "Handling Non-Linear Separability:\n",
    "    In cases where the data is not linearly separable in the original feature space, the support vectors play a crucial role when kernel functions are used.\n",
    "    Kernel functions allow SVMs to map the data into a higher-dimensional space where it might become linearly separable. \n",
    "    In the higher-dimensional space, support vectors still play their central role in defining the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd43c8a6-c8c0-4d5e-86b9-b4c2e598a802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89768179-2ad3-49ea-a214-d1afcdc01085",
   "metadata": {},
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55130ec3-c784-4ef1-8fd8-5ffe202bfb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 5:-\n",
    "Sure, lets visualize the concepts of hyperplane, marginal plane, soft margin, and hard margin in Support Vector Machines (SVM) using some simple examples and graphs. \n",
    "Well use a 2D feature space for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fb0ecf-69e9-48bf-9d1c-3e53d4a8b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scenario 1: Hyperplane and Hard Margin (C=∞)\n",
    "In this scenario, we have a \"hard margin\" SVM, which means the SVM does not tolerate any data points within the margin. \n",
    "It tries to find the largest possible margin that separates the two classes.\n",
    "Lets see the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761c797c-aa86-4e7a-b054-cec9f74ffe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "# Data\n",
    "X = np.array([[1, 3], [2, 2], [2, 4], [4, 2], [5, 3], [5, 1])\n",
    "y = [1, 1, 1, -1, -1, -1]\n",
    "\n",
    "# Create an SVM classifier with a linear kernel (Use a large value for C for a hard margin)\n",
    "clf = svm.SVC(kernel='linear', C=1e10)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n",
    "\n",
    "# Get the separating hyperplane (hyperplane and margin)\n",
    "w = clf.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(0, 6)\n",
    "yy = a * xx - clf.intercept_[0] / w[1]\n",
    "\n",
    "# Margin lines\n",
    "b = clf.support_vectors_[0]\n",
    "yy_down = a * xx + (b[1] - a * b[0])\n",
    "b = clf.support_vectors_[-1]\n",
    "yy_up = a * xx + (b[1] - a * b[0])\n",
    "\n",
    "plt.plot(xx, yy, 'k-')\n",
    "plt.plot(xx, yy_down, 'k--')\n",
    "plt.plot(xx, yy_up, 'k--')\n",
    "\n",
    "# Plot support vectors\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100, facecolors='none', edgecolors='k')\n",
    "plt.title('Hard Margin SVM')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dfda91-6a2a-4557-8acd-0e59cc2e7c26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f802c72-7fb1-4ac0-a99f-fa20a1aed339",
   "metadata": {},
   "source": [
    "Q6. SVM Implementation through Iris dataset.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83459c25-8c4c-4917-801e-3bd5f78b679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6:-\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "clf = svm.SVC(kernel='linear', C=1)  # You can experiment with different values of C\n",
    "clf.fit(X_train[:, :2], y_train)  # Using only two features for visualization\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred = clf.predict(X_test[:, :2])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Create a mesh to plot decision boundaries\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Plot decision boundaries\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.title('SVM Decision Boundaries (C=1)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b508b5b6-0d9b-420d-92b7-0d66febdd74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5564ca-694a-4305-86cb-4dd68ce0fe05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
