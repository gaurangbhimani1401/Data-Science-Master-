{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e60d69-b603-40bf-a370-a92c0691fb98",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f119a2d-38cf-4d8c-b66a-6d9f7f152f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 1:-\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale numerical features to a specific range.\n",
    "It transforms the feature values to a new range [min, max], typically [0, 1], making the data comparable and improving the performance of certain machine learning\n",
    "algorithms that are sensitive to the scale of input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba317a-cbe7-4c90-bdd9-f869347cddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_x = (x - min(x)) / (max(x) - min(x))\n",
    "Suppose you have a dataset with a feature representing the age of people ranging from 20 to 60 years.\n",
    "The age feature has the following values: [20, 30, 40, 50, 60]. You want to scale these values to the range [0, 1] using Min-Max scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d9888-cf4e-45ae-ac06-3deab9016180",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(age) = 20\n",
    "max(age) = 60\n",
    "\n",
    "scaled_age(20) = (20 - 20) / (60 - 20) = 0\n",
    "scaled_age(30) = (30 - 20) / (60 - 20) = 0.1\n",
    "scaled_age(40) = (40 - 20) / (60 - 20) = 0.4\n",
    "scaled_age(50) = (50 - 20) / (60 - 20) = 0.6\n",
    "scaled_age(60) = (60 - 20) / (60 - 20) = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d660d3b-8193-4829-89eb-6f9ce5b17b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f31d5da2-f5e5-4abb-9455-60b32bd884cd",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8408e-e69e-4876-8c42-1eda7db9eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 2:-\n",
    "The Unit Vector technique, also known as Vector Normalization or L2 Normalization, is a feature scaling method used to transform data such that each sample (row) in\n",
    "the dataset has a length of 1.\n",
    "It is mainly used to normalize the magnitude of data vectors to ensure that they have the same scale without changing their direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6bb2d7-9839-4628-b058-9daf3ada6c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_vector_x = x / ||x||\n",
    "where x is the original feature vector, ||x|| represents the L2 norm of x, which is calculated as the square root of the sum of the squared elements of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe662f5-a798-441a-a6d2-24a8a380481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "height  weight\n",
    "---------------\n",
    "  170    60\n",
    "  185    80\n",
    "  160    50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6707c-3e90-4ad8-8558-40d52dec50b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "||[170, 60]|| = sqrt(170^2 + 60^2) ≈ 181.90\n",
    "Similarly, for the other rows:\n",
    "||[185, 80]|| ≈ 198.07\n",
    "||[160, 50]|| ≈ 167.33\n",
    "unit_vector_row1 = [170, 60] / 181.90 ≈ [0.935, 0.354]\n",
    "\n",
    "Similarly, for the other rows:\n",
    "unit_vector_row2 = [185, 80] / 198.07 ≈ [0.932, 0.363]\n",
    "unit_vector_row3 = [160, 50] / 167.33 ≈ [0.956, 0.292]\n",
    "After applying Unit Vector scaling, each row now has a length (L2 norm) of approximately 1, and the direction of the original vectors is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7024c-dc1f-4060-990a-eb2ddd66d99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2be568f8-e43d-4158-8933-b00fb46b1c87",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e2c48-a17c-431a-b8f7-5b6667b0ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 3:-\n",
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving\n",
    "the most important patterns and variances in the original data.\n",
    "It does this by finding the principal components, which are orthogonal vectors that represent the directions of maximum variance in the data. \n",
    "These principal components are used to construct a new set of uncorrelated features (principal components) that capture the most significant information in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67046a-c6d1-4f88-b54e-fb6a8420c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "Consider a dataset of 2D points representing the following 4 data samples:\n",
    "(2, 3)\n",
    "(4, 5)\n",
    "(6, 7)\n",
    "(8, 9)\n",
    "\n",
    "Step 1: Data Standardization:\n",
    "mean(Feature1) = (2 + 4 + 6 + 8) / 4 = 5\n",
    "mean(Feature2) = (3 + 5 + 7 + 9) / 4 = 6\n",
    "\n",
    "std(Feature1) = sqrt(((2-5)^2 + (4-5)^2 + (6-5)^2 + (8-5)^2) / 4) ≈ 2.58\n",
    "std(Feature2) = sqrt(((3-6)^2 + (5-6)^2 + (7-6)^2 + (9-6)^2) / 4) ≈ 2.58\n",
    "\n",
    "(-1.15, -1.15)\n",
    "(-0.58, -0.58)\n",
    "(0.58, 0.58)\n",
    "(1.15, 1.15)\n",
    "\n",
    "Step 2: Covariance Matrix:\n",
    "\n",
    "         | 1.5   1.5 |\n",
    "Cov =    |           |\n",
    "         | 1.5   1.5 |\n",
    "\n",
    "\n",
    "Step 3: Eigenvectors and Eigenvalues:\n",
    "\n",
    "[  0.707 ]\n",
    "[        ]\n",
    "[  0.707 ]\n",
    "\n",
    "Step 4: Dimensionality Reduction:\n",
    "\n",
    "(2, 3)   * [0.707] = 2.12\n",
    "(4, 5)   * [0.707] = 4.24\n",
    "(6, 7)   * [0.707] = 6.36\n",
    "(8, 9)   * [0.707] = 8.48\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4daf70-5987-419c-89d8-cff988b165e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0321c464-a891-4027-a0bf-d8942c0fd76e",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a15de1-94b6-4d0c-b40d-3db4d1eff5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 4:-\n",
    "PCA (Principal Component Analysis) is a dimensionality reduction technique that can also be used for feature extraction.\n",
    "In feature extraction, the goal is to transform the original features into a new set of features that capture the most relevant information in the data while\n",
    "reducing dimensionality.\n",
    "PCA achieves this by finding the principal components, which are linear combinations of the original features that represent the directions of maximum variance\n",
    "in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888677b6-e756-4a5b-9d18-68216fab2041",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dimensionality Reduction:\n",
    "    PCA reduces the dimensionality of the data by projecting it onto a lower-dimensional subspace defined by the principal components.\n",
    "    The number of principal components retained determines the dimensionality of the new feature space.\n",
    "\n",
    "Information Compression:\n",
    "    PCA compresses the data by representing it in terms of the principal components, which are orthogonal and uncorrelated.\n",
    "    These components are ranked based on the amount of variance they explain, allowing for data compression while preserving most of the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52af324-44ab-4b8e-ad09-2d2b64be086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Age  | Income | Spending\n",
    "------------------------\n",
    " 30  | 50000  | 1000\n",
    " 40  | 60000  | 1500\n",
    " 25  | 30000  | 800\n",
    "Step 1: Data Standardization:\n",
    "Standardize the data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Step 2: Compute Covariance Matrix:\n",
    "Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "Step 3: Eigenvectors and Eigenvalues:\n",
    "Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "Step 4: Selection of Principal Components:\n",
    "Select the top k eigenvectors and eigenvalues. For example, if we want to reduce the data to two dimensions, we would select the top two eigenvectors with the\n",
    "highest eigenvalues.\n",
    "\n",
    "Step 5: Dimensionality Reduction:\n",
    "Project the original data onto the selected principal components to obtain the new feature space with reduced dimensions.\n",
    "\n",
    "PC1  |  PC2\n",
    "-----------\n",
    " 1    |  -0.5\n",
    " 0    |   1.5\n",
    "-1    |  -1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbd25e-d6db-48eb-8afc-7753e8cc67d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e7292fa-4bde-43b2-813f-b324cf502db4",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57c463-1d3f-41dd-a01b-19648c21f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 5:-\n",
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used as a data preprocessing technique to standardize the\n",
    "numerical features in the dataset.\n",
    "The purpose of Min-Max scaling is to transform the original features into a specific range, typically [0, 1], by linearly scaling the values based on their minimum\n",
    "and maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b6c805-1138-405e-a532-c30f795a4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apply Min-Max scaling: For each numerical feature, apply the Min-Max scaling formula:\n",
    "\n",
    "scaled_value = (original_value - min_value) / (max_value - min_value)\n",
    "\n",
    "This formula linearly scales each value to the range [0, 1].\n",
    "\n",
    "Price (USD) | Rating (out of 5) | Delivery Time (minutes)\n",
    "-------------------------------------------------------\n",
    "   10.0     |        4.5       |         30\n",
    "   15.0     |        4.0       |         45\n",
    "   8.0      |        4.8       |         25\n",
    "    \n",
    "min(Price) = 8.0\n",
    "max(Price) = 15.0\n",
    "\n",
    "min(Rating) = 4.0\n",
    "max(Rating) = 4.8\n",
    "\n",
    "min(Delivery Time) = 25\n",
    "max(Delivery Time) = 45\n",
    "\n",
    "Scaled Price = (Price - 8.0) / (15.0 - 8.0)\n",
    "Scaled Rating = (Rating - 4.0) / (4.8 - 4.0)\n",
    "Scaled Delivery Time = (Delivery Time - 25) / (45 - 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b671ea6-ccc9-43ed-bdaf-f4b6a4714f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87b56c67-a0ac-406c-972a-3070faab64cd",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ceba52-e5ec-47f0-a930-7473daaa336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 6:-\n",
    "In the project to predict stock prices, PCA (Principal Component Analysis) can be used to reduce the dimensionality of the dataset, which is beneficial when\n",
    "dealing with a large number of features.\n",
    "Dimensionality reduction with PCA helps in simplifying the data representation while retaining the most significant patterns and variances in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7baf512-8c4c-4d96-b253-904f0a5b0f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Preprocessing:\n",
    "    Before applying PCA, it is essential to preprocess the data by handling missing values, standardizing numerical features, and encoding categorical variables if\n",
    "    any.\n",
    "\n",
    "Standardization:\n",
    "    Since PCA is sensitive to the scale of the features, it is crucial to standardize the numerical features to have zero mean and unit variance.\n",
    "\n",
    "Covariance Matrix:\n",
    "    Compute the covariance matrix of the standardized dataset.\n",
    "    The covariance matrix represents the relationships between all pairs of features and their respective variances.\n",
    "\n",
    "Eigenvectors and Eigenvalues:\n",
    "    Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "    The eigenvectors represent the principal components, and the corresponding eigenvalues indicate the amount of variance explained by each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea69625-5ab9-4a4d-a99a-c67728a94430",
   "metadata": {},
   "outputs": [],
   "source": [
    "By applying PCA, the new feature space will contain fewer dimensions (k) than the original dataset, and each dimension will represent a combination of the original\n",
    "features.\n",
    "The reduced feature space can then be used for model training and prediction.\n",
    "\n",
    "Advantages of using PCA for dimensionality reduction in stock price prediction:\n",
    "\n",
    "Reduced Complexity:\n",
    "    PCA simplifies the dataset by removing less informative dimensions, leading to a more interpretable model.\n",
    "\n",
    "Noise Reduction:\n",
    "    By retaining the principal components with the highest variance, PCA can reduce noise in the data, improving model performance.\n",
    "\n",
    "Avoiding Overfitting:\n",
    "    Dimensionality reduction with PCA helps in reducing the risk of overfitting, especially when dealing with a large number of features.\n",
    "\n",
    "Improved Computational Efficiency:\n",
    "    Working with a lower-dimensional feature space can speed up the training and prediction process for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80c465-d2cd-47d3-94b3-81e996e0c1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e10fe281-67d5-4b57-a605-c94e258300cc",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f502e9-d8ab-4aa6-9d69-db6054bd0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 7:-\n",
    "To perform Min-Max scaling and transform the values to a range of -1 to 1, we can use the following formula:\n",
    "scaled_value = (x - min_value) / (max_value - min_value) * (new_max - new_min) + new_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ceec89-2108-4515-bf71-044525e893c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this case, the original dataset is [1, 5, 10, 15, 20], and we want to scale the values to the range of -1 to 1.\n",
    "\n",
    "Step 1: Calculate min_value and max_value from the original dataset:\n",
    "\n",
    "min_value = 1\n",
    "max_value = 20\n",
    "\n",
    "Step 2: Determine the new_min and new_max for the scaled range:\n",
    "\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "Step 3: Perform Min-Max scaling for each value in the dataset:\n",
    "\n",
    "For x = 1:\n",
    "scaled_value = (1 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 0\n",
    "\n",
    "For x = 5:\n",
    "scaled_value = (5 - 1) / (20 - 1) * (1 - (-1)) + (-1) = -0.5\n",
    "\n",
    "For x = 10:\n",
    "scaled_value = (10 - 1) / (20 - 1) * (1 - (-1)) + (-1) = -0.1111...\n",
    "\n",
    "For x = 15:\n",
    "scaled_value = (15 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 0.3333...\n",
    "\n",
    "For x = 20:\n",
    "scaled_value = (20 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 1\n",
    "\n",
    "The scaled values for the dataset [1, 5, 10, 15, 20] in the range of -1 to 1 are:\n",
    "\n",
    "[0, -0.5, -0.1111..., 0.3333..., 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8631b3b-0cbd-40ba-88e0-6ce729d5de31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cd0dbb3-2a09-4393-8b5f-0c0ac6c4b8f4",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2d0256-c0bd-4294-9d62-75e53192c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 8:-\n",
    "For feature extraction using PCA, the goal is to reduce the dimensionality of the dataset while retaining as much variance as possible.\n",
    "The number of principal components to retain depends on the amount of variance we want to preserve in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f394e-db03-4fa7-8568-aeb26c7c3e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Preprocessing:\n",
    "    Before applying PCA, we need to preprocess the data, including handling missing values, encoding categorical variables (e.g., gender), and standardizing numerical\n",
    "    features (height, weight, age, blood pressure).\n",
    "\n",
    "Covariance Matrix:\n",
    "    Calculate the covariance matrix of the standardized dataset.\n",
    "    The covariance matrix shows the relationships between the features and their respective variances.\n",
    "\n",
    "Eigenvectors and Eigenvalues:\n",
    "    Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "    The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "Cumulative Explained Variance:\n",
    "    Sort the eigenvectors in descending order based on their corresponding eigenvalues.\n",
    "    Calculate the cumulative explained variance, which represents the proportion of total variance explained by each principal component as we add them sequentially.\n",
    "\n",
    "Choose the Number of Principal Components:\n",
    "    Decide the number of principal components to retain based on the cumulative explained variance.\n",
    "    A common approach is to choose the number of components that explain a sufficiently high percentage of the total variance (e.g., 95% or 99%)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
